{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Softmax Regression* (synonyms: *Multinomial Logistic*, *Maximum Entropy Classifier*, or just *Multi-class Logistic Regression*) is a generalization of logistic regression that we can use for multi-class classification (under the assumption that the classes are  mutually exclusive). \n",
    "\n",
    "The schematic diagram of (standard) *Logistic Regression* model in binary classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](logistic_regression_schematic_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The schematic diagram for Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](softmax_schematic_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *Softmax Regression*, we replace the sigmoid logistic function by the so-called *softmax* function $\\phi_{softmax}(\\cdot)$.\n",
    "\n",
    "$$P(y=i \\mid z^{(i)}) = \\phi_{softmax}(z^{(i)}) = \\frac{e^{z^{(i)}}}{\\sum_{j=0}^{k} e^{z_{j}^{(i)}}},$$\n",
    "\n",
    "where we define the net input *z* as \n",
    "\n",
    "$$z = w_1x_1 + ... + w_mx_m  + b= \\sum_{l=0}^{m} w_l x_l + b= \\mathbf{w}^T\\mathbf{x} + b.$$ \n",
    "\n",
    "(**w** is the weight vector, $\\mathbf{x}$ is the feature vector of 1 training sample, and $b$ is the bias unit.)   \n",
    "Now, this softmax function computes the probability that this training sample $\\mathbf{x}^{(i)}$ belongs to class $j$ given the weight and net input $z^{(i)}$. So, we compute the probability $p(y = j \\mid \\mathbf{x^{(i)}; w}_j)$ for each class label in  $j = 1, \\ldots, k.$. Note the normalization term in the denominator which causes these class probabilities to sum up to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assuming that the input is vectorised and in the form of X = [samples,features]\n",
    "and the target labels are y with actual labels 0,1,..9. and the splits are trainX, trainY, testX and testY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to define a function for the softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to compute the softmax activation that we discussed earlier:\n",
    "\n",
    "$$\\phi_{softmax}(z) = \\frac{e^{z}}{\\sum_{j=0}^{k} e^{z_{k}}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ex = np.exp(x-np.max(x))\n",
    "    deno = np.sum(ex, axis=1).reshape(-1,1)\n",
    "    a = np.divide(ex,deno)\n",
    "    return np.divide(ex, deno)\n",
    "    # write the code for softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load one batch\n",
    "def getData(file) :\n",
    "    name = './cifar-10-batches-py/'+file\n",
    "    with open(name, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding=\"bytes\")\n",
    "    return data# function to load one batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loading train-validation data\n",
    "X = []\n",
    "y = []\n",
    "for i in range(5) :\n",
    "    filename = \"data_batch_\"+str(i+1)\n",
    "    data = getData(filename)\n",
    "    X.append(data[b'data'])\n",
    "    y.append(data[b'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input of size (50000, 3072) and output of size (50000,1)\n",
    "X = np.array(X).reshape(-1,3072)\n",
    "X = np.divide(X,255)\n",
    "y = np.array(y).reshape(50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading test data\n",
    "Xtest = getData(\"test_batch\")[b'data']\n",
    "Ytest = np.array(getData(\"test_batch\")[b'labels'])\n",
    "Xtest = np.divide(Xtest,255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meenu/.local/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "label_names = [lbl.decode(\"utf-8\") for lbl in getData(\"batches.meta\")[b'label_names']]\n",
    "\n",
    "[xtrain,xtest,ytrain,ytest] = train_test_split(X,y,random_state=42,train_size=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to apply one-hot encoding to the target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotIt(Y):\n",
    "    yHot = np.zeros((Y.shape[0],10))\n",
    "#     print(yHot.shape)\n",
    "    index = 0\n",
    "    for i in Y :\n",
    "        yHot[index][i] = 1\n",
    "        index += 1\n",
    "    return yHot\n",
    "    # write the code to convert Y into one-hot encoded labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax uses Cross-entropy loss to learn the weights.\n",
    "\n",
    "$$J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum_{i=1}^{n} H(T_i, O_i),$$\n",
    "\n",
    "which is the average of all cross-entropies over our $n$ training samples. The cross-entropy  function is defined as\n",
    "\n",
    "$$H(T_i, O_i) = -\\sum_m T_i \\cdot log(O_i).$$\n",
    "\n",
    "Here the $T$ stands for \"target\" (i.e., the *true* class labels) and the $O$ stands for output -- the computed *probability* via softmax; **not** the predicted class label.\n",
    "\n",
    "In order to learn our softmax model -- determining the weight coefficients -- via gradient descent, we then need to compute the derivative of the loss,\n",
    "\n",
    "$$\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b}).$$\n",
    "\n",
    "we won't walk through the tedious details here, but this cost derivative turns out to be simply:\n",
    "\n",
    "$$\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum^{n}_{i=0} \\big[\\mathbf{x}^{(i)}\\ \\big(O_i - T_i \\big) \\big]$$\n",
    "\n",
    "We can then use the cost derivate to update the weights in opposite direction of the cost gradient with learning rate $\\eta$:\n",
    "\n",
    "$$\\mathbf{w}_j := \\mathbf{w}_j - \\eta \\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b})$$ \n",
    "\n",
    "for each class $$j \\in \\{0, 1, ..., k\\}$$\n",
    "\n",
    "(note that $\\mathbf{w}_j$ is the weight vector for the class $y=j$), and we update the bias units\n",
    "\n",
    "\n",
    "$$\\mathbf{b}_j := \\mathbf{b}_j   - \\eta \\bigg[ \\frac{1}{n} \\sum^{n}_{i=0} \\big(O_i - T_i  \\big) \\bigg].$$ \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a penalty against complexity, an approach to reduce the variance of our model and decrease the degree of overfitting by adding additional bias, we can further add a regularization term such as the L2 term with the regularization parameter $\\lambda$:\n",
    "    \n",
    "L2:        $\\frac{\\lambda}{2} ||\\mathbf{w}||_{2}^{2}$, \n",
    "\n",
    "where \n",
    "\n",
    "$$||\\mathbf{w}||_{2}^{2} = \\sum^{m}_{l=0} \\sum^{k}_{j=0} w_{i, j}$$\n",
    "\n",
    "so that our cost function becomes\n",
    "\n",
    "$$J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum_{i=1}^{n} H(T_i, O_i) + \\frac{\\lambda}{2} ||\\mathbf{w}||_{2}^{2}$$\n",
    "\n",
    "and we define the \"regularized\" weight update as\n",
    "\n",
    "$$\\mathbf{w}_j := \\mathbf{w}_j -  \\eta \\big[\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}) + \\lambda \\mathbf{w}_j \\big].$$\n",
    "\n",
    "(Please note that we don't regularize the bias term.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function getLoss takes in the weights w, the input x, they targets y and a regularising coefficient lam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLoss(w,x,y,lam=0):\n",
    "    m = x.shape[0]#First we get the number of training examples\n",
    "    y_mat = oneHotIt(y) #Next we convert the integer class coding into a one-hot representation using the oneHotit function\n",
    "    y_true = np.argmax(np.array(y),axis=1)\n",
    "    scores = np.matmul(x,w)#Then we compute raw class scores given our input and current weights (w*x)\n",
    "    prob = softmax(scores) #Next we perform a softmax on these scores to get their probabilities\n",
    "    sample = 0\n",
    "    totalLoss = 0\n",
    "    \n",
    "    for category in y_true :\n",
    "        totalLoss += (-np.log(prob[sample][category]))\n",
    "        sample += 1 \n",
    "    loss = totalLoss/sample #We then find the loss of the probabilities\n",
    "    grad = (-1 / m) * np.dot(x.T,(y_mat - prob)) + lam*w #And compute the gradient for that loss\n",
    "    return loss,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbsAndPreds(someX):\n",
    "    probs = softmax(np.dot(someX,w))\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return probs,preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = xtrain\n",
    "trainY = ytrain.reshape(-1,1)\n",
    "w = np.zeros([trainX.shape[1],len(np.unique(trainY))]) ## initialise the weights\n",
    "lam = 0\n",
    "iterations = 100 ## mention the number of iterations \n",
    "learningRate = 1e-2\n",
    "losses = []\n",
    "for i in range(0,iterations):\n",
    "    loss,grad = getLoss(w, trainX, trainY ,lam)\n",
    "    losses.append(loss)\n",
    "    w = w - (learningRate * grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(someX,someY):\n",
    "    prob,prede = getProbsAndPreds(someX)\n",
    "    accuracy = sum(prede == someY)/(float(len(someY)))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.323725\n",
      "Validation Accuracy:  0.316\n",
      "Test Accuracy:  0.3202\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy: ', getAccuracy(xtrain,ytrain))\n",
    "print('Validation Accuracy: ', getAccuracy(xtest,ytest))\n",
    "\n",
    "print('Test Accuracy: ', getAccuracy(Xtest,Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accuracy of both softmax regression and one vs rest for 10 classifiers have less accuracy when compared to 3 \n",
    "# class classifiers. this is because the model is not complex enough and it underfits the given data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
